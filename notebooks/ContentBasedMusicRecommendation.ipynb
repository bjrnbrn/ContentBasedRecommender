{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automatic Playlist Generation:\n",
    "# A Content-Based Music Sequence Recommender System"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Concept\n",
    "\n",
    "#### A. Podcast-like Playlists:\n",
    "- Categorical Tags (Genre, Era/Year, Label, Producers)\n",
    "- Qualitative Tags (Dancebility, BPM, Key, Vocal/Instrumental)\n",
    "\n",
    "#### B. Mixtape-like Playlists:  \n",
    "- Audio Features\n",
    "- Feature Similarity Measures   \n",
    "    - Harmony\n",
    "    - Rhythm\n",
    "    - Sound\n",
    "    - Instrumentation\n",
    "    - Mood/Sentiment\n",
    "    - Dynamic\n",
    "\n",
    "#### C. DJ-Mixes:\n",
    "- Start/Intro & End/Outro Features of Songs\n",
    "- Beat Matching Features\n",
    "- Song to Song Transition Features\n",
    "- Story Telling Features over whole Song Sequence    \n",
    "- Coherence Measures for Transitions & Sequences"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Recommender Systems Overview: State of the Art Approaches\n",
    "\n",
    "#### A. Two General Approaches:\n",
    "\n",
    "- **Collaborative filtering:** Matrix Factorization, alternating least squares\n",
    "- **Content-based approaches:** Input is music information (basis of songs and/or \n",
    "    existing playlists) fetched through Music Information Retrieval (MIR) processes\n",
    "\n",
    "\n",
    " B. What are the Recommendations / the generated Playlists based on?\n",
    "\n",
    "- emotion / mood\n",
    "- genre\n",
    "- user taste\n",
    "- user similarity\n",
    "- popularity\n",
    "\n",
    "\n",
    " C. More recent Approaches / Deep Learning Approaches\n",
    "\n",
    "- Sequence-aware music recommendation:\n",
    "    - Next track recommendatons\n",
    "    - Automatic playlist continuation (APC)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Possible Datasets, Models & Feature Selection\n",
    "\n",
    "  A. Datasets:\n",
    "\n",
    "[**Melon Music Dataset**](https://github.com/MTG/melon-music-dataset)  \n",
    "[last.fm Dataset](https://zenodo.org/record/6090214)  \n",
    "[MTG Barcelona Datasets & Software](https://www.upf.edu/web/mtg/software-datasets)  \n",
    "[Kaggle: Spotify Tracks Dataset](https://www.kaggle.com/datasets/maharshipandya/-spotify-tracks-dataset?datasetId=2570056&sortBy=voteCount)  \n",
    "[Kaggle: Spotify Playlists Dataset](https://www.kaggle.com/datasets/andrewmvd/spotify-playlists?datasetId=1720572&sortBy=voteCount)\n",
    "\n",
    "  B. Python Audio Analysis (MIR) Packages: \n",
    "\n",
    "[**Essentia (ML Application ready)**](https://essentia.upf.edu/)  \n",
    "[Essentia citing papers](https://essentia.upf.edu/research_papers.html)  \n",
    "[**Librosa (lightweigth analysis)**](https://librosa.org/doc/main/feature.html)\n",
    "\n",
    "\n",
    "  C. Youtube Tutorials:\n",
    "\n",
    "[Spotify Playlist Generation](https://www.youtube.com/watch?v=3vvvjdmBoyc&list=PL-wATfeyAMNrTEgZyfF66TwejA0MRX7x1&index=2)  \n",
    "[Librosa Music Analysis](https://www.youtube.com/watch?v=MhOdbtPhbLU)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Content-Based Recommendation\n",
    "\n",
    "**Reasoning:** *Cold-start problem for metadata-based recommendation systems using only user-generated metadata*  \n",
    "**Solution:** *Find underlying features of audio/music by MIR*  \n",
    "**High-level Features:** *genre, mood, instrument(s), vocals, gender of singing voice, lyrics, ...*  \n",
    "**Low-level Features:** *MFCC, ZCR, Spectral Coefficients, mixability*\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Strategy\n",
    "\n",
    "    1. Obtain mixes data\n",
    "\n",
    "    2. Get songs for each mix  \n",
    "\n",
    "    3. Analyze song / sequence data for content-based Recommendation system  \n",
    "\n",
    "    4. Produce playlists  \n",
    "\n",
    "    5. Compare to baseline model  \n",
    "\n",
    "    (6. Produce dj-mix with transitions)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "map mixes songs to spotify\n",
    "\n",
    "extract items featurers matrix for mixes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import librosa\n",
    "import librosa.display\n",
    "\n",
    "from IPython.display import Audio\n",
    "import ipywidgets as widgets"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  Librosa: MIR Library"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  The Chromagram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_harmonic, y_percussive = librosa.effects.hpss(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# C = librosa.feature.chroma_cqt(y=y_harmonic, sr=sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(16,4))\n",
    "# librosa.display.specshow(C, sr=sr, x_axis='time', y_axis='chroma', vmin=0, vmax=1)\n",
    "# plt.title('Chromagram for: {}'.format(song_name))\n",
    "# plt.colorbar()\n",
    "# plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### mixesDb Extractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from eda import Mixes, Tracks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mixes = Mixes('../data/djmix-dataset.json', min_files=5, max_files=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mixes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mixes.groupby('genre').mix_id.count()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QUESTIONS:\n",
    "        can I use this data to categorize other songs?\n",
    "        This genre-list is pretty long, maybe it might be \n",
    "        usefull for genre classification beforehand.\n",
    "#### MAKE TRACKS DB WITH GENRES\n",
    "        label all tracks with mix_ids, genres, what else?\n",
    "#### USER-ITEM-LIKE MATRIX FOR MIXES AND TRACKS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Librosa Library: Features\n",
    "\n",
    "**Spectral Features:**\n",
    "\n",
    "1. `chroma_stft`: Computes the chromagram (pitch class profile) from a waveform or power spectrogram. Useful for music genre classification and chord recognition.\n",
    "\n",
    "2. `chroma_cqt`: Computes the constant-Q chromagram, which is a chromagram variant based on the constant-Q transform. It is useful for analyzing musical audio.\n",
    "\n",
    "3. `chroma_cens`: Computes the chroma variant called \"Chroma Energy Normalized\" (CENS). It provides a more robust representation of tonal content and is useful for tasks like cover song identification.\n",
    "\n",
    "4. `chroma_vqt`: Computes the variable-Q chromagram, which is a chromagram variant based on the variable-Q transform. It is useful for analyzing musical audio.\n",
    "\n",
    "5. `melspectrogram`: Computes a mel-scaled spectrogram, which represents the power spectrum of a signal in mel-frequency bins. It is commonly used in speech and music analysis.\n",
    "\n",
    "6. `mfcc`: Computes Mel-frequency cepstral coefficients (MFCCs), which are a compact representation of the spectral envelope of a signal. They are widely used in speech and audio processing tasks such as speech recognition and speaker identification.\n",
    "\n",
    "7. `rms`: Computes the root-mean-square (RMS) value for each frame in an audio signal. It provides a measure of the overall energy level of the signal.\n",
    "\n",
    "8. `spectral_centroid`: Computes the spectral centroid, which represents the center of mass of the spectrum. It provides information about the \"brightness\" of a sound and is useful for tasks like instrument recognition.\n",
    "\n",
    "9. `spectral_bandwidth`: Computes the spectral bandwidth, which measures the spread of the spectrum around the spectral centroid. It provides information about the \"width\" of a sound and is useful for tasks like timbre characterization.\n",
    "\n",
    "10. `spectral_contrast`: Computes spectral contrast, which measures the difference in amplitude between peaks and valleys in the spectrum. It is useful for tasks like music genre classification and instrument recognition.\n",
    "\n",
    "11. `spectral_flatness`: Computes the spectral flatness, which quantifies the \"tonality\" of a sound by measuring the ratio of the geometric mean to the arithmetic mean of the spectrum.\n",
    "\n",
    "12. `spectral_rolloff`: Computes the roll-off frequency, which is the frequency below which a specified percentage of the total spectral energy lies. It provides information about the shape and brightness of the spectrum.\n",
    "\n",
    "13. `poly_features`: Computes the coefficients of fitting an nth-order polynomial to the columns of a spectrogram. It can be used for tasks like pitch estimation and separation of harmonic and percussive components.\n",
    "\n",
    "14. `tonnetz`: Computes the tonal centroid features (tonnetz), which represent the harmonic relationships between musical notes. It is useful for tasks like chord recognition and key estimation.\n",
    "\n",
    "15. `zero_crossing_rate`: Computes the zero-crossing rate, which measures the rate at which a signal changes sign. It is often used as a simple feature for tasks like speech and music onset detection.\n",
    "\n",
    "\n",
    "**Rhythm Features:**\n",
    "\n",
    "16. `tempo`: Estimates the tempo (beats per minute) of an audio signal. It is useful for tasks like music classification and tempo-based audio processing.\n",
    "\n",
    "17. `tempogram`: Computes the tempogram, which is a local autocorrelation of the onset strength envelope. It provides a visualization of rhythmic patterns in the audio signal.\n",
    "\n",
    "18. `fourier_tempogram`: Computes the Fourier tempogram, which is the short-time Fourier transform of the onset strength envelope. It provides a spectrogram-like representation of rhythmic patterns in the audio signal.\n",
    "\n",
    "19. `tempogram_ratio`: Computes temp"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Brain Storming\n",
    "    Song Feature Dataframe\n",
    "    Mix Feature Matrix\n",
    "    Model Selection\n",
    "    INPUT OUTPUT: 2-3 Songs -> recoomend existing mix\n",
    "    HAT ARE FEATURES THAT MAKE A SONG FIT INTO A PLAYLIST? -> TRACK SILMILIARITY FEATURES\n",
    "    WHAT ARE FEATURES THAT ORDER A PLAYLIST TO A SEQUENCE? -> MIX SEQUENCE FEATURES, TRANSITION MATRIX?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## REVERSE ENGINEERING OF MIX FEATURES: TIMESERIES TRENDS\n",
    "    HOW DO THE FEATURES EVOLVE OVER A SET?\n",
    "    HOW DO I WANT THE FEATURES TO EVOLVE OVER A SET?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BASELINE MODEL: BPM FILTER & RANDOM SAMPLING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_file = '../audio/Ezra - Garten meiner Fantasie.wav'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_file_name = init_file.split('/')[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_file_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y, sr = librosa.load(init_file)\n",
    "\n",
    "# Estimate BPM\n",
    "init_tempo, _ = librosa.beat.beat_track(y=y, sr=sr)\n",
    "\n",
    "print(\"Estimated Starting BPM:\", init_tempo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_tempo = init_tempo * 0.9\n",
    "min_tempo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_tempo = init_tempo * 1.0825\n",
    "max_tempo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_folder = '../audio/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import librosa\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synchable_files = []\n",
    "for file in os.listdir(files_folder):\n",
    "    try:\n",
    "        y, sr = librosa.load(''.join([files_folder,file]))\n",
    "        tempo, _ = librosa.beat.beat_track(y=y, sr=sr)\n",
    "        synchable_files.append([file, tempo])\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "possible_files = pd.DataFrame(synchable_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "possible_files.columns=['file', 'bpm']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_sorted = possible_files.sort_values(by='bpm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_files = files_sorted[(files_sorted['bpm'] >= min_tempo) & (files_sorted['bpm'] <= max_tempo)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_sample = filtered_files.query(\"file != @init_file_name\").sample(n=12).sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_sample = filtered_sample.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_row = pd.DataFrame({'file': [init_file_name], 'bpm': [init_tempo]})\n",
    "filtered_sample = pd.concat([init_row, filtered_sample], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_bpm = filtered_sample.bpm.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_sample.bpm.median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_sample.bpm.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyrubberband as pyrb\n",
    "import soundfile as sf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_tempo(input_file, output_file, target_bpm):\n",
    "    y, sr = librosa.load(input_file)\n",
    "    tempo, beat_frames = librosa.beat.beat_track(y=y, sr=sr)\n",
    "    current_bpm = librosa.beat.tempo(y=y, sr=sr, hop_length=512)[0]\n",
    "    time_stretch_ratio = target_bpm / current_bpm\n",
    "    y_stretched = librosa.effects.time_stretch(y=y, rate=time_stretch_ratio)\n",
    "    sf.write(output_file, y_stretched, sr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in filtered_sample['file']:\n",
    "    try:\n",
    "        input_file = ''.join([files_folder, file])\n",
    "        output_file = ''.join([files_folder, 'bpm_adjusted_', file])\n",
    "        adjust_tempo(input_file, output_file, target_bpm)\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bpm_adjusted_files = []\n",
    "for file in os.listdir(files_folder):\n",
    "    try:\n",
    "        if file.startswith('bpm_adjusted'):\n",
    "            y, sr = librosa.load(''.join([files_folder,file]))\n",
    "            tempo, _ = librosa.beat.beat_track(y=y, sr=sr)\n",
    "            bpm_adjusted_files.append([file, tempo])\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bpm_adjusted_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import madmom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
